#!/usr/bin/env python3
"""
å¿«é€ŸORB-SLAM3 AIæ¨¡å‹è®­ç»ƒè„šæœ¬
ç”¨äºå°æ•°æ®é›†çš„å¿«é€ŸåŸå‹è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
import json
import os
from typing import Dict, List
import logging
from datetime import datetime

class QuickSLAMDataset(Dataset):
    """å¿«é€ŸSLAMæ•°æ®é›†"""
    
    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.samples = self._load_samples()
        
    def _load_samples(self) -> List[Dict]:
        """åŠ è½½è®­ç»ƒæ ·æœ¬"""
        samples = []
        data_file = os.path.join(self.data_dir, 'training_data.json')
        
        if os.path.exists(data_file):
            with open(data_file, 'r') as f:
                samples = json.load(f)
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = os.path.join(self.data_dir, sample['image_path'])
        image = cv2.imread(image_path)
        if image is None:
            # åˆ›å»ºè™šæ‹Ÿå›¾åƒä½œä¸ºå¤‡ç”¨
            image = np.random.randint(0, 255, (240, 320, 3), dtype=np.uint8)
        else:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ç®€å•çš„å›¾åƒé¢„å¤„ç†
        image = cv2.resize(image, (64, 64))  # æ›´å°çš„å°ºå¯¸ç”¨äºå¿«é€Ÿè®­ç»ƒ
        image = image.astype(np.float32) / 255.0
        image = np.transpose(image, (2, 0, 1))  # CHWæ ¼å¼
        
        # ç¯å¢ƒç‰¹å¾
        env_features = np.array(sample['env_features'], dtype=np.float32)
        
        # ç›®æ ‡å‚æ•°
        target_params = np.array(sample['optimal_params'], dtype=np.float32)
        
        return {
            'image': torch.tensor(image),
            'env_features': torch.tensor(env_features),
            'target_params': torch.tensor(target_params),
            'performance_score': sample['performance_score']
        }

class SimpleSLAMNetwork(nn.Module):
    """ç®€åŒ–çš„SLAMå‚æ•°é¢„æµ‹ç½‘ç»œ"""
    
    def __init__(self, env_feature_dim=8, output_dim=8):
        super().__init__()
        
        # ç®€å•çš„CNNç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4)),
            nn.Flatten(),
            nn.Linear(64 * 4 * 4, 128),
            nn.ReLU()
        )
        
        # å‚æ•°é¢„æµ‹ç½‘ç»œ
        self.param_predictor = nn.Sequential(
            nn.Linear(128 + env_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, output_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )
    
    def forward(self, image, env_features):
        # æå–å›¾åƒç‰¹å¾
        img_features = self.feature_extractor(image)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([img_features, env_features], dim=1)
        
        # é¢„æµ‹å‚æ•°è°ƒæ•´
        param_adjustments = self.param_predictor(combined_features)
        
        return param_adjustments

def quick_train():
    """å¿«é€Ÿè®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¿«é€ŸAIæ¨¡å‹è®­ç»ƒ...")
    
    # è®¾ç½®
    device = torch.device('cpu')  # ä½¿ç”¨CPUé¿å…GPUè®¾ç½®é—®é¢˜
    data_dir = '/home/wb/ros_ws/slam_training_data'
    model_save_dir = '/home/wb/ros_ws/src/orb_slam3_ai/models/trained'
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(model_save_dir, exist_ok=True)
    
    # æ•°æ®é›†
    dataset = QuickSLAMDataset(data_dir)
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")
    
    if len(dataset) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®")
        return
    
    # æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    # æ¨¡å‹
    model = SimpleSLAMNetwork().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒå¾ªç¯
    num_epochs = 50  # å¿«é€Ÿè®­ç»ƒï¼Œè¾ƒå°‘epoch
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch in dataloader:
            images = batch['image'].to(device)
            env_features = batch['env_features'].to(device)
            target_params = batch['target_params'].to(device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            predicted_params = model(images, env_features)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(predicted_params, target_params)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(model_save_dir, 'quick_slam_model.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'env_feature_dim': 8,
            'output_dim': 8,
            'model_type': 'SimpleSLAMNetwork'
        },
        'training_info': {
            'epochs': num_epochs,
            'final_loss': avg_loss,
            'dataset_size': len(dataset),
            'timestamp': datetime.now().isoformat()
        }
    }, model_path)
    
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    print(f"ğŸ“Š æœ€ç»ˆæŸå¤±: {avg_loss:.6f}")
    
    # æµ‹è¯•æ¨¡å‹
    print("\nğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")
    model.eval()
    with torch.no_grad():
        test_sample = dataset[0]
        test_image = test_sample['image'].unsqueeze(0).to(device)
        test_env = test_sample['env_features'].unsqueeze(0).to(device)
        test_target = test_sample['target_params']
        
        prediction = model(test_image, test_env)
        
        print(f"ç›®æ ‡å‚æ•°: {test_target.numpy()}")
        print(f"é¢„æµ‹å‚æ•°: {prediction.squeeze().cpu().numpy()}")
        print(f"é¢„æµ‹è¯¯å·®: {torch.mean(torch.abs(prediction.squeeze().cpu() - test_target)).item():.6f}")

if __name__ == '__main__':
    quick_train()