#!/usr/bin/env python3
"""
ORB-SLAM3 AIè®­ç»ƒæ•°æ®é›†ä¸‹è½½è„šæœ¬
ä»å¼€æºç½‘ç«™ä¸‹è½½æ ‡å‡†SLAMæ•°æ®é›†ç”¨äºè®­ç»ƒ
"""

import os
import sys
import wget
import tarfile
import zipfile
import cv2
import numpy as np
import json
from pathlib import Path
import shutil
from typing import Dict, List, Tuple
import argparse

class DatasetDownloader:
    """SLAMæ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, base_dir="/home/wb/ros_ws/slam_training_data"):
        self.base_dir = Path(base_dir)
        self.datasets_dir = self.base_dir / "raw_datasets"
        self.processed_dir = self.base_dir / "processed"
        
        # åˆ›å»ºç›®å½•
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # TUM RGB-Dæ•°æ®é›†URLs
        self.tum_datasets = {
            'freiburg1_xyz': 'https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_xyz.tgz',
            'freiburg1_desk': 'https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk.tgz',
            'freiburg2_xyz': 'https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_xyz.tgz',
            'freiburg2_desk': 'https://vision.in.tum.de/rgbd/dataset/freiburg2/rgbd_dataset_freiburg2_desk.tgz',
            'freiburg3_office': 'https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household.tgz'
        }
        
        # EuRoC MAVæ•°æ®é›†URLs (é€‰æ‹©éƒ¨åˆ†æœ‰ä»£è¡¨æ€§çš„åºåˆ—)
        self.euroc_datasets = {
            'MH_01_easy': 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/machine_hall/MH_01_easy/MH_01_easy.zip',
            'MH_03_medium': 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/machine_hall/MH_03_medium/MH_03_medium.zip',
            'V1_01_easy': 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.zip',
            'V2_01_easy': 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room2/V2_01_easy/V2_01_easy.zip'
        }
        
        print(f"ğŸ“ æ•°æ®é›†å°†ä¸‹è½½åˆ°: {self.base_dir}")
    
    def download_file(self, url: str, filename: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶"""
        filepath = self.datasets_dir / filename
        
        if filepath.exists():
            print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨: {filename}")
            return True
            
        try:
            print(f"ğŸ”„ ä¸‹è½½ä¸­: {filename}")
            print(f"   URL: {url}")
            
            # ä½¿ç”¨wgetä¸‹è½½æ–‡ä»¶
            wget.download(url, str(filepath))
            print(f"\\nâœ… ä¸‹è½½å®Œæˆ: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {filename} - {e}")
            if filepath.exists():
                filepath.unlink()
            return False
    
    def extract_archive(self, filepath: Path, extract_dir: Path) -> bool:
        """è§£å‹æ–‡ä»¶"""
        try:
            print(f"ğŸ“¦ è§£å‹ä¸­: {filepath.name}")
            
            if filepath.suffix == '.tgz' or filepath.name.endswith('.tar.gz'):
                with tarfile.open(filepath, 'r:gz') as tar:
                    tar.extractall(extract_dir)
            elif filepath.suffix == '.zip':
                with zipfile.ZipFile(filepath, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath.suffix}")
                return False
                
            print(f"âœ… è§£å‹å®Œæˆ: {filepath.name}")
            return True
            
        except Exception as e:
            print(f"âŒ è§£å‹å¤±è´¥: {filepath.name} - {e}")
            return False
    
    def download_tum_datasets(self, datasets: List[str] = None):
        """ä¸‹è½½TUM RGB-Dæ•°æ®é›†"""
        print("\\nğŸ¤– å¼€å§‹ä¸‹è½½TUM RGB-Dæ•°æ®é›†...")
        
        if datasets is None:
            datasets = list(self.tum_datasets.keys())
        
        tum_dir = self.datasets_dir / "TUM_RGB-D"
        tum_dir.mkdir(exist_ok=True)
        
        for dataset_name in datasets:
            if dataset_name not in self.tum_datasets:
                print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
                continue
                
            url = self.tum_datasets[dataset_name]
            filename = f"{dataset_name}.tgz"
            
            # ä¸‹è½½
            if self.download_file(url, filename):
                # è§£å‹
                filepath = self.datasets_dir / filename
                self.extract_archive(filepath, tum_dir)
    
    def download_euroc_datasets(self, datasets: List[str] = None):
        """ä¸‹è½½EuRoC MAVæ•°æ®é›†"""
        print("\\nğŸš å¼€å§‹ä¸‹è½½EuRoC MAVæ•°æ®é›†...")
        
        if datasets is None:
            datasets = list(self.euroc_datasets.keys())
        
        euroc_dir = self.datasets_dir / "EuRoC_MAV"
        euroc_dir.mkdir(exist_ok=True)
        
        for dataset_name in datasets:
            if dataset_name not in self.euroc_datasets:
                print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
                continue
                
            url = self.euroc_datasets[dataset_name]
            filename = f"{dataset_name}.zip"
            
            # ä¸‹è½½
            if self.download_file(url, filename):
                # è§£å‹
                filepath = self.datasets_dir / filename
                self.extract_archive(filepath, euroc_dir)
    
    def process_tum_dataset(self, dataset_name: str) -> List[Dict]:
        """å¤„ç†TUMæ•°æ®é›†æ ¼å¼"""
        print(f"ğŸ”„ å¤„ç†TUMæ•°æ®é›†: {dataset_name}")
        
        dataset_dir = None
        for d in (self.datasets_dir / "TUM_RGB-D").iterdir():
            if d.is_dir() and dataset_name in d.name:
                dataset_dir = d
                break
        
        if not dataset_dir:
            print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•: {dataset_name}")
            return []
        
        print(f"ğŸ“ å¤„ç†ç›®å½•: {dataset_dir}")
        
        # è¯»å–å…³è”æ–‡ä»¶
        associations_file = dataset_dir / "associations.txt"
        if not associations_file.exists():
            # ç”Ÿæˆå…³è”æ–‡ä»¶
            self.generate_tum_associations(dataset_dir)
        
        samples = []
        rgb_dir = dataset_dir / "rgb"
        depth_dir = dataset_dir / "depth"
        
        if not rgb_dir.exists() or not depth_dir.exists():
            print(f"âŒ RGBæˆ–æ·±åº¦ç›®å½•ä¸å­˜åœ¨")
            return []
        
        # è¯»å–å‰100ä¸ªæ ·æœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•
        rgb_files = sorted(list(rgb_dir.glob("*.png")))[:100]
        
        for i, rgb_file in enumerate(rgb_files):
            # æŸ¥æ‰¾å¯¹åº”çš„æ·±åº¦å›¾åƒ
            timestamp = rgb_file.stem
            depth_file = depth_dir / f"{timestamp}.png"
            
            if not depth_file.exists():
                continue
            
            # åˆ†æå›¾åƒç‰¹å¾
            rgb_img = cv2.imread(str(rgb_file))
            if rgb_img is None:
                continue
                
            env_features = self.analyze_image_features(rgb_img)
            
            # æ ¹æ®æ•°æ®é›†ç‰¹æ€§è®¾ç½®ç¯å¢ƒç±»å‹
            env_type = self.classify_tum_environment(dataset_name, i)
            optimal_params = self.get_optimal_params_for_env(env_type)
            
            sample = {
                "dataset": "TUM_RGB-D",
                "sequence": dataset_name,
                "image_path": f"TUM_RGB-D/{dataset_dir.name}/rgb/{rgb_file.name}",
                "depth_path": f"TUM_RGB-D/{dataset_dir.name}/depth/{depth_file.name}",
                "env_features": env_features,
                "optimal_params": optimal_params,
                "performance_score": 0.85 + np.random.random() * 0.1,  # æ¨¡æ‹Ÿæ€§èƒ½åˆ†æ•°
                "scene_type": env_type,
                "motion_type": "mixed"
            }
            
            samples.append(sample)
        
        print(f"âœ… å¤„ç†å®Œæˆ: {len(samples)} ä¸ªæ ·æœ¬")
        return samples
    
    def analyze_image_features(self, image: np.ndarray) -> List[float]:
        """åˆ†æå›¾åƒç‰¹å¾"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # äº®åº¦
        brightness = np.mean(gray) / 255.0
        
        # å¯¹æ¯”åº¦
        contrast = np.std(gray) / 255.0
        
        # çº¹ç†å¯†åº¦ (ä½¿ç”¨Sobelè¾¹ç¼˜æ£€æµ‹)
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        texture_density = np.mean(edge_magnitude) / 255.0
        
        # ORBç‰¹å¾ç‚¹æ•°é‡
        orb = cv2.ORB_create()
        keypoints = orb.detect(gray, None)
        feature_count = len(keypoints) / 1000.0  # å½’ä¸€åŒ–
        
        # æ¨¡æ‹Ÿè¿åŠ¨æ•°æ®
        motion_magnitude = np.random.random() * 0.5
        angular_velocity = np.random.random() * 0.3
        
        # è¾¹ç¼˜å¯†åº¦
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        
        # ç¨³å®šæ€§è¯„åˆ† (åŸºäºå›¾åƒè´¨é‡)
        stability = min(1.0, contrast * 2.0)
        
        return [
            brightness,
            contrast, 
            texture_density,
            motion_magnitude,
            angular_velocity,
            edge_density,
            feature_count,
            stability
        ]
    
    def classify_tum_environment(self, dataset_name: str, frame_idx: int) -> str:
        """æ ¹æ®TUMæ•°æ®é›†åç§°å’Œå¸§ç´¢å¼•åˆ†ç±»ç¯å¢ƒ"""
        if 'desk' in dataset_name:
            return 'high_texture'
        elif 'xyz' in dataset_name:
            return 'normal'
        elif 'office' in dataset_name:
            return 'low_light'
        else:
            return 'normal'
    
    def get_optimal_params_for_env(self, env_type: str) -> List[float]:
        """è·å–ç¯å¢ƒå¯¹åº”çš„æœ€ä¼˜SLAMå‚æ•°"""
        param_configs = {
            'low_texture': [600, 1.1, 6, 0.85, 0.8, 0.2, 0.1, 2],
            'high_texture': [1200, 1.25, 8, 0.7, 0.9, 0.3, 0.05, 4],
            'low_light': [500, 1.15, 6, 0.8, 0.75, 0.25, 0.08, 3],
            'fast_motion': [1000, 1.2, 7, 0.75, 0.85, 0.35, 0.06, 3],
            'normal': [1000, 1.2, 8, 0.75, 0.9, 0.25, 0.05, 3]
        }
        
        base_params = param_configs.get(env_type, param_configs['normal'])
        
        # æ·»åŠ ä¸€äº›éšæœºå™ªå£°æ¥å¢åŠ æ•°æ®å¤šæ ·æ€§
        noisy_params = []
        for param in base_params:
            noise = np.random.normal(0, 0.05)  # 5%çš„å™ªå£°
            noisy_params.append(max(0.1, param * (1 + noise)))
        
        return noisy_params
    
    def generate_tum_associations(self, dataset_dir: Path):
        """ä¸ºTUMæ•°æ®é›†ç”Ÿæˆå…³è”æ–‡ä»¶"""
        rgb_dir = dataset_dir / "rgb"
        depth_dir = dataset_dir / "depth"
        
        if not rgb_dir.exists() or not depth_dir.exists():
            return
        
        rgb_files = sorted(rgb_dir.glob("*.png"))
        depth_files = sorted(depth_dir.glob("*.png"))
        
        associations = []
        for rgb_file in rgb_files:
            # æŸ¥æ‰¾æœ€è¿‘çš„æ·±åº¦æ–‡ä»¶
            rgb_timestamp = float(rgb_file.stem)
            best_depth = None
            min_diff = float('inf')
            
            for depth_file in depth_files:
                depth_timestamp = float(depth_file.stem)
                diff = abs(rgb_timestamp - depth_timestamp)
                if diff < min_diff:
                    min_diff = diff
                    best_depth = depth_file
            
            if best_depth and min_diff < 0.02:  # 20mså†…çš„åŒ¹é…
                associations.append(f"{rgb_timestamp} rgb/{rgb_file.name} {float(best_depth.stem)} depth/{best_depth.name}")
        
        # ä¿å­˜å…³è”æ–‡ä»¶
        with open(dataset_dir / "associations.txt", 'w') as f:
            f.write("\\n".join(associations))
    
    def create_training_dataset(self):
        """åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†"""
        print("\\nğŸ¯ åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        
        all_samples = []
        
        # å¤„ç†TUMæ•°æ®é›†
        tum_dir = self.datasets_dir / "TUM_RGB-D"
        if tum_dir.exists():
            for dataset_dir in tum_dir.iterdir():
                if dataset_dir.is_dir():
                    dataset_name = dataset_dir.name.split('_')[-1] if '_' in dataset_dir.name else dataset_dir.name
                    samples = self.process_tum_dataset(dataset_name)
                    all_samples.extend(samples)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        output_file = self.base_dir / "training_data.json"
        with open(output_file, 'w') as f:
            json.dump(all_samples, f, indent=2)
        
        print(f"âœ… è®­ç»ƒæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"   æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        print(f"   æ•°æ®æ–‡ä»¶: {output_file}")
        
        # åˆ›å»ºæ•°æ®é›†ç»Ÿè®¡
        self.create_dataset_statistics(all_samples)
        
        return all_samples
    
    def create_dataset_statistics(self, samples: List[Dict]):
        """åˆ›å»ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_samples": len(samples),
            "datasets": {},
            "scene_types": {},
            "avg_features": {
                "brightness": 0,
                "contrast": 0,
                "texture_density": 0
            }
        }
        
        # ç»Ÿè®¡ä¸åŒæ•°æ®é›†
        for sample in samples:
            dataset = sample["dataset"]
            scene_type = sample["scene_type"]
            
            stats["datasets"][dataset] = stats["datasets"].get(dataset, 0) + 1
            stats["scene_types"][scene_type] = stats["scene_types"].get(scene_type, 0) + 1
            
            # ç´¯è®¡ç‰¹å¾å€¼
            features = sample["env_features"]
            stats["avg_features"]["brightness"] += features[0]
            stats["avg_features"]["contrast"] += features[1] 
            stats["avg_features"]["texture_density"] += features[2]
        
        # è®¡ç®—å¹³å‡å€¼
        for key in stats["avg_features"]:
            stats["avg_features"][key] /= len(samples)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.base_dir / "dataset_statistics.json"
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   æ•°æ®é›†åˆ†å¸ƒ: {stats['datasets']}")
        print(f"   åœºæ™¯ç±»å‹åˆ†å¸ƒ: {stats['scene_types']}")

def main():
    parser = argparse.ArgumentParser(description='ä¸‹è½½SLAMè®­ç»ƒæ•°æ®é›†')
    parser.add_argument('--base-dir', default='/home/wb/ros_ws/slam_training_data',
                        help='æ•°æ®é›†ä¿å­˜ç›®å½•')
    parser.add_argument('--tum-only', action='store_true',
                        help='åªä¸‹è½½TUMæ•°æ®é›†')
    parser.add_argument('--euroc-only', action='store_true', 
                        help='åªä¸‹è½½EuRoCæ•°æ®é›†')
    parser.add_argument('--quick', action='store_true',
                        help='å¿«é€Ÿæ¨¡å¼ï¼Œåªä¸‹è½½éƒ¨åˆ†æ•°æ®é›†')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = DatasetDownloader(args.base_dir)
    
    print("ğŸš€ å¼€å§‹ä¸‹è½½SLAMè®­ç»ƒæ•°æ®é›†...")
    print("="*50)
    
    try:
        if args.quick:
            # å¿«é€Ÿæ¨¡å¼ï¼šåªä¸‹è½½å°æ•°æ®é›†
            print("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä¸‹è½½ç²¾é€‰æ•°æ®é›†")
            downloader.download_tum_datasets(['freiburg1_xyz', 'freiburg1_desk'])
        elif args.tum_only:
            # åªä¸‹è½½TUMæ•°æ®é›†
            downloader.download_tum_datasets()
        elif args.euroc_only:
            # åªä¸‹è½½EuRoCæ•°æ®é›†
            downloader.download_euroc_datasets()
        else:
            # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
            downloader.download_tum_datasets(['freiburg1_xyz', 'freiburg1_desk', 'freiburg2_xyz'])
            # downloader.download_euroc_datasets(['MH_01_easy'])  # EuRoCæ•°æ®é›†è¾ƒå¤§ï¼Œå…ˆæ³¨é‡Š
        
        # å¤„ç†æ•°æ®é›†
        print("\\n" + "="*50)
        training_samples = downloader.create_training_dataset()
        
        print("\\nğŸ‰ æ•°æ®é›†ä¸‹è½½å’Œå¤„ç†å®Œæˆ!")
        print(f"ğŸ“ æ•°æ®ä½ç½®: {downloader.base_dir}")
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(training_samples)}")
        
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        print("\\nğŸ“– ä½¿ç”¨è¯´æ˜:")
        print(f"1. è®­ç»ƒæ•°æ®: {downloader.base_dir}/training_data.json")
        print(f"2. åŸå§‹æ•°æ®: {downloader.base_dir}/raw_datasets/")
        print(f"3. å¼€å§‹è®­ç»ƒ: python3 src/orb_slam3_ai/scripts/train_ai_model.py")
        
    except KeyboardInterrupt:
        print("\\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        print(f"\\nâŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()