#!/usr/bin/env python3
"""
ORB-SLAM3 AIæ¨¡å‹è®­ç»ƒè„šæœ¬
ç”¨äºè®­ç»ƒè‡ªå®šä¹‰çš„SLAMå‚æ•°ä¼˜åŒ–æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
import json
import os
from typing import Dict, List, Tuple
import logging
from datetime import datetime

class SLAMDataset(Dataset):
    """SLAMè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = self._load_samples()
        
    def _load_samples(self) -> List[Dict]:
        """åŠ è½½è®­ç»ƒæ ·æœ¬"""
        samples = []
        data_file = os.path.join(self.data_dir, 'training_data.json')
        
        if os.path.exists(data_file):
            with open(data_file, 'r') as f:
                samples = json.load(f)
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = os.path.join(self.data_dir, sample['image_path'])
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            image = self.transform(image)
        
        # ç¯å¢ƒç‰¹å¾
        env_features = torch.tensor(sample['env_features'], dtype=torch.float32)
        
        # ç›®æ ‡å‚æ•°
        target_params = torch.tensor(sample['optimal_params'], dtype=torch.float32)
        
        return {
            'image': image,
            'env_features': env_features,
            'target_params': target_params,
            'performance_score': sample['performance_score']
        }

class SLAMParameterNetwork(nn.Module):
    """SLAMå‚æ•°é¢„æµ‹ç½‘ç»œ"""
    
    def __init__(self, feature_dim=512, env_feature_dim=8, output_dim=8):
        super().__init__()
        
        # å›¾åƒç‰¹å¾æå–å™¨ï¼ˆä½¿ç”¨é¢„è®­ç»ƒResNet18ï¼‰
        import torchvision.models as models
        self.backbone = models.resnet18(pretrained=True)
        self.backbone.fc = nn.Identity()
        
        # å†»ç»“backboneçš„å‰å‡ å±‚
        for param in list(self.backbone.parameters())[:-20]:
            param.requires_grad = False
        
        # å‚æ•°é¢„æµ‹ç½‘ç»œ
        self.param_predictor = nn.Sequential(
            nn.Linear(feature_dim + env_feature_dim, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.2),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            
            nn.Linear(128, output_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )
    
    def forward(self, image, env_features):
        # æå–å›¾åƒç‰¹å¾
        img_features = self.backbone(image)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([img_features, env_features], dim=1)
        
        # é¢„æµ‹å‚æ•°è°ƒæ•´
        param_adjustments = self.param_predictor(combined_features)
        
        return param_adjustments

class SLAMTrainer:
    """SLAM AIæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_save_dir: str):
        self.model_save_dir = model_save_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SLAMParameterNetwork().to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=1e-5
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=10, factor=0.5
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()
        
        # æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
    def train_epoch(self, dataloader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        
        for batch_idx, batch in enumerate(dataloader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = batch['image'].to(self.device)
            env_features = batch['env_features'].to(self.device)
            target_params = batch['target_params'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predicted_params = self.model(images, env_features)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predicted_params, target_params)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 10 == 0:
                self.logger.info(f'Batch {batch_idx}, Loss: {loss.item():.6f}')
        
        return total_loss / len(dataloader)
    
    def validate(self, dataloader: DataLoader) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in dataloader:
                images = batch['image'].to(self.device)
                env_features = batch['env_features'].to(self.device)
                target_params = batch['target_params'].to(self.device)
                
                predicted_params = self.model(images, env_features)
                loss = self.criterion(predicted_params, target_params)
                total_loss += loss.item()
        
        return total_loss / len(dataloader)
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              num_epochs: int = 100):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 20
        
        for epoch in range(num_epochs):
            self.logger.info(f'Epoch {epoch+1}/{num_epochs}')
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step(val_loss)
            
            self.logger.info(f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.save_model(f'best_model_epoch_{epoch+1}.pth')
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= max_patience:
                self.logger.info(f'Early stopping at epoch {epoch+1}')
                break
    
    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.model_save_dir, filename)
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': {
                'feature_dim': 512,
                'env_feature_dim': 8,
                'output_dim': 8
            }
        }, model_path)
        
        self.logger.info(f'Model saved to {model_path}')

def collect_training_data():
    """æ•°æ®æ”¶é›†æŒ‡å—"""
    guide = """
    ğŸ“Š ORB-SLAM3 AIæ¨¡å‹è®­ç»ƒæ•°æ®æ”¶é›†æŒ‡å—
    
    1. æ•°æ®æ”¶é›†ç¯å¢ƒ
    ================
    - å¤šç§ç¯å¢ƒ: å®¤å†…ã€å®¤å¤–ã€ä¸åŒå…‰ç…§æ¡ä»¶
    - å¤šç§çº¹ç†: é«˜çº¹ç†ã€ä½çº¹ç†ã€é‡å¤çº¹ç†
    - å¤šç§è¿åŠ¨: æ…¢é€Ÿã€å¿«é€Ÿã€é™æ­¢ã€æ—‹è½¬
    
    2. æ•°æ®æ ¼å¼
    ===========
    training_data/
    â”œâ”€â”€ images/           # RGBå›¾åƒ
    â”œâ”€â”€ depth/            # æ·±åº¦å›¾åƒ
    â”œâ”€â”€ ground_truth/     # çœŸå®è½¨è¿¹
    â”œâ”€â”€ slam_results/     # SLAMè¾“å‡ºç»“æœ
    â””â”€â”€ training_data.json # è®­ç»ƒæ ‡ç­¾
    
    3. æ ‡ç­¾ç”Ÿæˆ
    ===========
    å¯¹æ¯ä¸ªåœºæ™¯ï¼š
    - è¿è¡ŒåŸç‰ˆORB-SLAM3ï¼Œè®°å½•æ€§èƒ½
    - æ‰‹åŠ¨è°ƒä¼˜å‚æ•°ï¼Œè·å¾—æœ€ä½³æ€§èƒ½
    - è®°å½•æœ€ä¼˜å‚æ•°ç»„åˆ
    - è®¡ç®—æ€§èƒ½è¯„åˆ†(è½¨è¿¹ç²¾åº¦ã€åœ°å›¾è´¨é‡)
    
    4. æ•°æ®æ ·æœ¬æ ¼å¼
    ===============
    {
        "image_path": "images/scene_001.jpg",
        "depth_path": "depth/scene_001.png", 
        "env_features": [brightness, contrast, texture_density, ...],
        "optimal_params": [nFeatures, scaleFactor, nLevels, ...],
        "performance_score": 0.95,
        "scene_type": "indoor_low_light",
        "motion_type": "slow_translation"
    }
    
    5. è®­ç»ƒå»ºè®®
    ===========
    - æ•°æ®é›†å¤§å°: 10,000+ æ ·æœ¬
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯• = 7:2:1
    - ä½¿ç”¨æ•°æ®å¢å¼º: å…‰ç…§å˜åŒ–ã€å™ªå£°æ·»åŠ 
    - å®šæœŸåœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯
    """
    return guide

if __name__ == '__main__':
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # æ‰“å°æ•°æ®æ”¶é›†æŒ‡å—
    print(collect_training_data())
    
    # ç¤ºä¾‹è®­ç»ƒä»£ç 
    print("\\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    
    # æ•°æ®è·¯å¾„
    data_dir = '/home/wb/ros_ws/slam_training_data'
    model_save_dir = '/home/wb/ros_ws/trained_models'
    
    if os.path.exists(data_dir):
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        from torchvision import transforms
        
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # æ•°æ®é›†
        full_dataset = SLAMDataset(data_dir, transform=transform)
        
        # åˆ’åˆ†æ•°æ®é›†
        train_size = int(0.8 * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        # è®­ç»ƒå™¨
        trainer = SLAMTrainer(model_save_dir)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(train_loader, val_loader, num_epochs=100)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
    else:
        print(f"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆæ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå‚è€ƒä¸Šé¢çš„æ•°æ®æ”¶é›†æŒ‡å—")